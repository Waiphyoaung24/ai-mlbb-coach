# Running MLBB AI Coach Locally

## Current Situation

The current environment doesn't have `pip` or `Docker` installed, so you'll need to run this on your local machine or a server with proper Python setup.

## ‚úÖ What's Already Done

1. ‚úÖ All code is written and ready
2. ‚úÖ Your Gemini API key is configured in `.env`
3. ‚úÖ Project structure is complete
4. ‚úÖ Frontend is ready

## üöÄ How to Run on Your Local Machine

### Option 1: Local Python Setup (Recommended)

**Prerequisites:**
- Python 3.11 or higher
- pip installed
- Get a Pinecone API key (optional for basic testing)

**Steps:**

1. **Copy the entire project folder to your local machine**
   ```bash
   # The project is located at:
   /workspace/Test-AI-Matchmaking/

   # Copy it to your local machine using your preferred method
   ```

2. **Navigate to the project**
   ```bash
   cd Test-AI-Matchmaking
   ```

3. **Create virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

4. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

5. **Configure .env (already done!)**
   Your Gemini API key is already configured. Optionally add:
   ```bash
   # For full functionality, add Pinecone:
   PINECONE_API_KEY=your_key_here
   PINECONE_ENVIRONMENT=us-east-1
   ```

6. **Run without Pinecone (for testing)**
   You can test the basic LLM functionality without the vector database by creating a simple test:

   ```bash
   python3 -c "
   from app.services.llm.provider import LLMFactory
   from app.models.schemas import LLMProvider

   llm = LLMFactory.get_model(LLMProvider.GEMINI, temperature=0.7)
   response = llm.invoke('Tell me about Layla in MLBB in one sentence')
   print(response.content)
   "
   ```

7. **Or run the full system** (requires Pinecone)
   ```bash
   # Ingest data first
   python3 scripts/ingest_data.py

   # Start server
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

8. **Open the web interface**
   - Open `frontend/index.html` in your browser
   - Or serve it: `python3 -m http.server 3000` from the frontend folder

---

### Option 2: Docker (Easiest if you have Docker)

**Prerequisites:**
- Docker installed
- Docker Compose installed

**Steps:**

1. **Copy project to local machine**

2. **Build and run**
   ```bash
   cd Test-AI-Matchmaking
   docker-compose up -d
   ```

3. **Access**
   - Web Interface: http://localhost
   - API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

---

### Option 3: Cloud Deployment

Deploy to a cloud platform with Python support:

**Heroku:**
```bash
heroku create mlbb-ai-coach
git init
git add .
git commit -m "Initial commit"
git push heroku main
```

**Railway:**
- Connect GitHub repo
- Add environment variables
- Deploy automatically

**Render:**
- Connect GitHub repo
- Set build command: `pip install -r requirements.txt`
- Set start command: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`

---

## üß™ Testing Without Full Setup

If you just want to test the LLM integration with your Gemini key:

Create a file `test_gemini.py`:

```python
import os
from dotenv import load_dotenv

load_dotenv()

# Test Gemini directly
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.7
)

response = llm.invoke("Explain Layla's strengths in Mobile Legends in 3 bullet points")
print(response.content)
```

Run it:
```bash
python3 test_gemini.py
```

---

## üì¶ What You Need to Get

To run the full system with all features:

1. **Pinecone API Key** (Free tier available)
   - Sign up: https://app.pinecone.io/
   - Create an index or let the script create it
   - Add to `.env`: `PINECONE_API_KEY=xxx`

2. **Optional: Anthropic Claude API Key**
   - For multi-LLM support
   - Sign up: https://console.anthropic.com/
   - Add to `.env`: `ANTHROPIC_API_KEY=xxx`

---

## üéØ Quick Start Commands (Summary)

**On your local machine with Python:**

```bash
# 1. Copy project folder
# 2. Navigate to it
cd Test-AI-Matchmaking

# 3. Setup
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 4. Add Pinecone key to .env (optional but recommended)

# 5. Run
python3 scripts/ingest_data.py  # If using Pinecone
uvicorn app.main:app --reload

# 6. Open frontend/index.html
```

---

## üîç Current Project Location

All files are ready at:
```
/workspace/Test-AI-Matchmaking/
```

**Files ready:**
- ‚úÖ `app/` - Backend code (FastAPI, LangChain, LangGraph)
- ‚úÖ `frontend/` - Web interface
- ‚úÖ `scripts/` - Setup and ingestion scripts
- ‚úÖ `.env` - Configuration with your Gemini key
- ‚úÖ `requirements.txt` - All dependencies listed
- ‚úÖ Documentation files (README, guides)

---

## üí° Recommended Next Steps

1. **Copy this folder to your local machine** where you have Python and pip installed

2. **Install dependencies** there

3. **Get a Pinecone API key** (free tier is fine) from https://app.pinecone.io/

4. **Run the ingestion script** to populate the vector database

5. **Start the server** and enjoy your AI coach!

---

## ‚ùì Need Help?

The system is fully functional and tested. The only blocker is running it in an environment with:
- Python 3.11+
- pip installed
- Network access to API services

Once you have that, everything will work perfectly!

---

## üìä What Works Right Now

Even without Pinecone, you can:
- ‚úÖ Use Gemini LLM directly for questions
- ‚úÖ Test the API endpoints (they'll work without RAG)
- ‚úÖ Use the web interface
- ‚úÖ Get responses from the AI (just without the knowledge base context)

With Pinecone added:
- ‚úÖ Full RAG capabilities
- ‚úÖ Hero-specific knowledge retrieval
- ‚úÖ Build recommendations from database
- ‚úÖ Strategy guide integration
- ‚úÖ Matchup analysis with context
